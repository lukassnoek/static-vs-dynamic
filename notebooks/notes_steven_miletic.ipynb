{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iraqi-failure",
   "metadata": {},
   "source": [
    "### The model is softmax\n",
    "\n",
    "$p(y_i) = \\frac{\\exp{\\beta x_i}}{\\sum_i \\exp{\\beta x_i}}$\n",
    "\n",
    "That is, we want to relate brain data $x$ of shape [K, F] to stimulus category $y$. The parameters are vector $\\beta$ of size F. Sampling can easily give you the uncertainty in $\\beta$ given the data and model (sort of using Bayes):\n",
    "\n",
    "$p(\\beta | x) = \\frac{p(x | \\beta) p(\\beta)}{ p(x) }$\n",
    "\n",
    "Suppose now we know the posterior distribution of $\\beta$. The posterior itself $\\beta$ is hard to interpret due to its dimensionality, so instead of directly interpreting the parameters we can try to understand how this model works by assessing for every $y$, which $x$ was most likely.\n",
    "\n",
    "The relation between $y, x, \\beta$ is given by softmax, again:\n",
    "\n",
    "$p(y_i) = \\frac{\\exp{\\beta x_i}}{\\sum_i \\exp{\\beta x_i}}$\n",
    "\n",
    "so if we want to go from $p(y)$ to $x$ instead of vice versa, we need to rearrange. Take log on both sides\n",
    "\n",
    "$\\ln(p(y_i)) = \\beta x_i - \\ln{\\sum_i \\beta x_i }$\n",
    "\n",
    "$\\ln(p(y_i)) + \\ln{\\sum_i \\beta x_i} = \\beta x_i$\n",
    "\n",
    "Since $\\ln{\\sum_i \\beta x_i}$ is constant for all $i$, write $\\ln{\\sum_i \\beta x_i} = c$\n",
    "\n",
    "$\\beta x_i = \\ln(p(y_i)) + c$\n",
    "\n",
    "$x_i = \\frac{\\ln(p(y_i)) + c}{\\beta} = \\frac{\\ln(p(y_i))}{\\beta} + c$\n",
    "\n",
    "Now I think you can randomly sample (say) 1e4 samples of $\\beta$ from your posterior, and for each sample, calculate for all $x_i$ for all $p(y_i) = 0.75$, $i \\in \\{1...K\\}$?\n",
    "\n",
    "The constant should (hopefully) not be so interesting and can (hopefully) be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "earlier-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "practical-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 800\n",
    "P = 2\n",
    "K = 6\n",
    "\n",
    "X = np.random.normal(0, 1, size=(N, P))\n",
    "α = np.random.normal(0, 1, size=K)\n",
    "β = np.random.normal(0, 1, size=(P, K))\n",
    "mu = X @ β + α\n",
    "p_y = softmax(mu, axis=1)\n",
    "\n",
    "# Make discrete labels\n",
    "y = np.array([np.random.multinomial(1, p_y[i, :]).argmax() for i in range(N)])\n",
    "x_ = np.random.normal(0, 1, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "breeding-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_ = softmax(x_ @ β + α)\n",
    "c = np.log(np.sum(np.exp(x_ @ β + α)))\n",
    "x_hat = (np.log(py_) - α + c) @ np.linalg.pinv(β)\n",
    "py_x_hat = softmax(x_hat @ β + α)\n",
    "np.testing.assert_almost_equal(py_, py_x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "diagnostic-forest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.29849923 -0.33633367]\n",
      "[ 1.29849923 -0.33633367]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(x_hat)\n",
    "print(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "mechanical-class",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-79a4c8661a77>:10: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample()\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [β_, α_]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8000' class='' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [8000/8000 01:59<00:00 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 120 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as logreg:\n",
    "    \n",
    "    α_ = pm.Normal('α_', 0, 1, shape=K)\n",
    "    β_ = pm.Normal('β_', 0, 1, shape=(P, K))\n",
    "    mu_ = tt.dot(X, β_) + α_\n",
    "    p_y_ = tt.nnet.softmax(mu_)\n",
    "    \n",
    "    # Do this separately for all classes [0, 1, 2, 3, 4, 5]\n",
    "    y_ = pm.Categorical('y_', p=p_y_, observed=y)\n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "remarkable-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.zeros((4000, 50))\n",
    "for i in range(4000):\n",
    "    β__ = trace['β_'][i, :, :]\n",
    "    α__ = trace['α_'][i, :]\n",
    "    X_[i, :] = (np.log([0.75, 0.05, 0.05, 0.05, 0.05, 0.05]) / (β__ + α__))[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "preliminary-smart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03555049, 0.07532734, 0.05801154, 0.1211306 , 0.37978935,\n",
       "       0.33019069])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "spectacular-language",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -14.34618063,   15.5755005 , -221.35196824,   10.73879958,\n",
       "         -6.29797071,  295.51203238,  -46.36097457,  -15.29234092,\n",
       "        -11.40047414,  -10.59289068,  -17.06979782,   45.2362775 ,\n",
       "         54.64527373,    9.69887371,   -5.19802335,  -15.52474307,\n",
       "         11.09071481,   -5.18853588,  -26.34683352,    9.00635544,\n",
       "        -47.24010665,   -7.17282345,   -6.28379333,   23.47053466,\n",
       "        -10.11187188,  -12.8351023 ,   12.99048311,    8.28103884,\n",
       "         49.54233168,   -9.06656821, -171.30894823, -138.14579647,\n",
       "         14.78052617, -295.735585  ,  138.17967526,  -13.80046704,\n",
       "         -7.9742874 ,   18.21827718,   -7.30681145,  -12.04135465,\n",
       "        -52.54368262,   -5.72279662,  -47.54592892,   -6.43465991,\n",
       "        -13.17473137,  -47.11568827,  -21.45841162,   10.64021153,\n",
       "         19.9040108 ,  -16.51002614])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "latter-intersection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00699804, 0.9235571 , 0.00000006, 0.00272476, 0.06514107,\n",
       "       0.00157897])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "softmax(X_.mean(axis=0) @ β + α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "noble-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt = (np.log([0.75, 0.05, 0.05, 0.05, 0.05, 0.05]) / (β + α))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "blond-technical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(X_opt[:, 5] @ β + α).round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
